"""
Analyser le rÃ´le de SWELU dans l'architecture MambaSWELU.
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent / "src"))

print("â•" * 80)
print("  ANALYSE: RÃ”LE DE SWELU DANS L'ARCHITECTURE")
print("â•" * 80)
print()

print("ğŸ—ï¸  ARCHITECTURE MAMBASWELU:")
print()
print("INPUT (tokens)")
print("  â†“")
print("Token Embedding + Position Embedding")
print("  â†“")
print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
print("6Ã— MAMBA BLOCKS (avec SWELU)")
print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
print()
print("  Chaque Mamba Block:")
print("    Input â†’ LayerNorm â†’ Mamba SSM â†’ SWELU â†’ Residual â†’ Output")
print("                            â†“")
print("                    SimplifiedMamba SSM:")
print("                      in_proj â†’ conv1d â†’ SWELU (ssm_activation)")
print("                                            â†“")
print("                                        out_proj")
print()
print("  ğŸ¯ SWELU dans Mamba:")
print("     - Remplace SiLU (activation originale de Mamba)")
print("     - AppliquÃ© APRÃˆS la sortie du SSM")
print("     - AppliquÃ© DANS le SSM (ssm_activation)")
print("     - Total: 2 SWELU par Mamba block â†’ 12 SWELU au total")
print()
print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
print("3Ã— DENSE LAYERS (avec SWELU)")
print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
print()
print("  Dense1: d_model â†’ dense_hidden_dim â†’ SWELU1 â†’ Dropout")
print("  Dense2: dense_hidden_dim â†’ dense_hidden_dim â†’ SWELU2 â†’ Dropout")
print("  Dense3: dense_hidden_dim â†’ d_model â†’ SWELU3")
print()
print("  ğŸ¯ SWELU dans Dense:")
print("     - Activation non-linÃ©aire entre couches")
print("     - Permet au modÃ¨le d'apprendre des transformations complexes")
print("     - Total: 3 SWELU")
print()
print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
print("  â†“")
print("LM Head (projection vers vocabulaire)")
print("  â†“")
print("OUTPUT (logits)")
print()
print("â•" * 80)
print()

print("ğŸ“Š TOTAL SWELU:")
print("   - Mamba blocks:     12 (6 layers Ã— 2 SWELU chacun)")
print("   - Dense layers:      3")
print("   - TOTAL:            15 paramÃ¨tres k apprenables")
print()

print("â•" * 80)
print("ğŸ¯ RÃ”LE DE SWELU vs SiLU:")
print("â•" * 80)
print()
print("  SiLU (original Mamba):")
print("    f(x) = x Ã— sigmoid(x)")
print("    - ParamÃ¨tres: AUCUN (fixe)")
print("    - Forme: FixÃ©e Ã  l'avance")
print()
print("  SWELU (votre innovation):")
print("    f(z, k) = sign(z) Ã— (1 - exp(-|z|^k))")
print("    - ParamÃ¨tres: k (APPRENABLE) âœ…")
print("    - Forme: S'ADAPTE aux donnÃ©es pendant l'entraÃ®nement")
print()
print("  ğŸ’¡ AVANTAGE:")
print("     Le modÃ¨le peut ADAPTER la forme de l'activation")
print("     Ã  chaque couche selon ce qui est optimal pour les donnÃ©es!")
print()

print("â•" * 80)
print("ğŸ§ª IMPACT SUR L'APPRENTISSAGE:")
print("â•" * 80)
print()
print("  Avec SiLU (activation fixe):")
print("    - Le modÃ¨le doit travailler AVEC la forme fixÃ©e de l'activation")
print("    - Certaines transformations peuvent Ãªtre difficiles")
print()
print("  Avec SWELU (activation adaptative):")
print("    - Chaque couche optimise SON PROPRE k")
print("    - k < 1: Activation plus linÃ©aire (gradients plus forts)")
print("    - k > 1: Activation plus non-linÃ©aire (plus de capacitÃ©)")
print("    - k â‰ˆ 1: Proche de l'exponentielle standard")
print()
print("  ğŸš€ RÃ‰SULTAT:")
print("     Le modÃ¨le a 15 degrÃ©s de libertÃ© supplÃ©mentaires")
print("     pour adapter ses activations de maniÃ¨re optimale!")
print()

print("â•" * 80)
print("ğŸ“ˆ PENDANT L'ENTRAÃNEMENT:")
print("â•" * 80)
print()
print("  Les valeurs de k vont Ã‰VOLUER:")
print("    - DÃ©but:  tous k = 1.0")
print("    - Warmup: k commence Ã  varier selon les gradients")
print("    - Later:  chaque k trouve sa valeur optimale")
print()
print("  Exemple attendu aprÃ¨s entraÃ®nement:")
print("    - PremiÃ¨res couches:  k â‰ˆ 0.7-0.9  (plus linÃ©aire)")
print("    - Couches moyennes:   k â‰ˆ 1.0-1.3  (Ã©quilibrÃ©)")
print("    - DerniÃ¨res couches:  k â‰ˆ 1.2-1.8  (plus non-linÃ©aire)")
print("    - Dense layers:       k variable selon la tÃ¢che")
print()

print("â•" * 80)
print("ğŸ’¡ CONCLUSION:")
print("â•" * 80)
print()
print("  âœ… OUI: SWELU k est bien dÃ©fini et apprenable")
print("  âœ… OUI: Le modÃ¨le apprend ces 15 paramÃ¨tres")
print("  âœ… OUI: Tous les SiLU sont remplacÃ©s par SWELU")
print()
print("  ğŸ¯ C'est exactement ce que tu voulais!")
print("     SWELU donne au modÃ¨le la flexibilitÃ© d'adapter")
print("     ses activations pendant l'entraÃ®nement.")
print()
print("â•" * 80)

