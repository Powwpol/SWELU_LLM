# üìä √âtude Comparative - 3 Mod√®les MambaSWELU

## üéØ Objectif

Comparer l'impact de **diff√©rents datasets** et **strat√©gies d'entra√Ænement** sur la performance du mod√®le MambaSWELU 124M.

---

## üî¨ Configuration de l'Exp√©rience

### Architecture Commune (Identique pour tous)
- **Mod√®le:** MambaSWELU
- **Param√®tres:** 124,104,719 (~124M)
- **Couches:** 6 Mamba blocks + 3 Dense layers
- **Dimension:** 1024
- **Activation:** SWELU (learnable)
- **Sequence length:** 1024
- **Batch size:** 4 (effective: 16 avec grad accum)
- **Learning rate:** 3e-4
- **Mixed precision:** BF16

### Variables (Ce qui change)

| Mod√®le | Dataset | Ratio | Steps | Tokens | GPUs | Dur√©e | Checkpoints |
|--------|---------|-------|-------|--------|------|-------|-------------|
| **1. SlimPajama LLaMA** | SlimPajama-627B | 100x | 757,500 | 12.4B | 0-1 | ~35h | ./checkpoints/slimpajama_llama/ |
| **2. SlimPajama Chinchilla** | SlimPajama-627B | 20x | 151,500 | 2.48B | 2-3 | ~7h | ./checkpoints/slimpajama_chinchilla/ |
| **3. Wikipedia LLaMA** | Wikipedia | 100x | 757,500 | 12.4B | 4-5 | ~35h | ./checkpoints/wikipedia_llama/ |

---

## üìà M√©triques √† Comparer

### 1. Loss (Perte d'entra√Ænement)
- Vitesse de convergence
- Loss finale
- Stabilit√© pendant l'entra√Ænement

### 2. Perplexit√©
- Sur validation set
- Sur test set
- Par type de texte

### 3. Qualit√© de G√©n√©ration
- Coh√©rence
- Diversit√©
- Factualit√©
- Cr√©ativit√©

### 4. Efficacit√©
- Temps d'entra√Ænement total
- Co√ªt GPU (GPU-heures)
- Ratio performance/co√ªt

---

## üîç Hypoth√®ses √† Tester

### H1: Impact du Dataset
**Question:** SlimPajama (627B tokens, diversifi√©) vs Wikipedia (plus petit, plus structur√©)

**Attentes:**
- SlimPajama ‚Üí Meilleure g√©n√©ralisation
- Wikipedia ‚Üí Meilleur sur texte encyclop√©dique

### H2: Impact du Ratio Tokens/Param√®tres
**Question:** Chinchilla (20x) vs LLaMA (100x)

**Attentes:**
- Chinchilla (20x) ‚Üí Convergence plus rapide, suffisant pour baseline
- LLaMA (100x) ‚Üí Meilleure performance finale

### H3: Dataset Quality vs Quantity
**Question:** Est-ce que la qualit√© de Wikipedia compense sa taille r√©duite?

**Attentes:**
- √Ä tokens √©gaux, Wikipedia pourrait √™tre comp√©titif
- SlimPajama devrait dominer avec 100x ratio

---

## üìä Timeline

```
Heure 0:    ‚ñà‚ñà‚ñà‚ñà Tous d√©marrent
Heure 7:    ‚ñà‚ñà‚ñà‚ñà Chinchilla termin√© ‚úì
Heure 35:   ‚ñà‚ñà‚ñà‚ñà LLaMA models termin√©s ‚úì
```

**Premier r√©sultat:** Chinchilla (SlimPajama) dans ~7h  
**R√©sultats finaux:** Dans ~35h

---

## üìù Logs et Monitoring

### Logs individuels
```bash
tail -f logs/slimpajama_llama.log
tail -f logs/slimpajama_chinchilla.log
tail -f logs/wikipedia_llama.log
```

### Monitoring global
```bash
./monitor_all_trainings.sh
watch -n 10 './monitor_all_trainings.sh'
```

### Checkpoints
```bash
ls -lh checkpoints/slimpajama_llama/
ls -lh checkpoints/slimpajama_chinchilla/
ls -lh checkpoints/wikipedia_llama/
```

---

## üß™ Protocole d'√âvaluation (Post-Entra√Ænement)

### 1. Perplexit√© Quantitative
```bash
# √âvaluer sur m√™me validation set
python eval.py --model checkpoints/slimpajama_llama/final_model.pt
python eval.py --model checkpoints/slimpajama_chinchilla/final_model.pt
python eval.py --model checkpoints/wikipedia_llama/final_model.pt
```

### 2. G√©n√©ration Qualitative
```bash
# M√™me prompt pour tous
PROMPT="The future of artificial intelligence is"
python generate.py --model slimpajama_llama --prompt "$PROMPT"
python generate.py --model slimpajama_chinchilla --prompt "$PROMPT"
python generate.py --model wikipedia_llama --prompt "$PROMPT"
```

### 3. Benchmarks Standards
- LAMBADA
- HellaSwag
- PIQA
- WinoGrande

---

## üí° Questions de Recherche

1. **Le ratio optimal est-il vraiment 20x (Chinchilla)?**
   - Ou 100x apporte-t-il des gains significatifs?

2. **SlimPajama justifie-t-il sa complexit√©?**
   - Vs Wikipedia qui est plus simple √† utiliser

3. **Pour un budget fixe, quelle strat√©gie?**
   - Chinchilla rapide pour it√©ration
   - LLaMA pour performance maximale

4. **Architecture Mamba + SWELU:**
   - Suit-elle les m√™mes lois d'√©chelle que Transformers?
   - Les gains de SWELU sont-ils dataset-d√©pendants?

---

## üìä R√©sultats Attendus

### Sc√©nario Optimiste
- **Chinchilla (7h):** Baseline acceptable rapidement
- **SlimPajama LLaMA:** Meilleur mod√®le overall
- **Wikipedia LLaMA:** Comp√©titif sur texte encyclop√©dique

### Insights Esp√©r√©s
1. Courbes de loss comparatives
2. Loi d'√©chelle pour Mamba+SWELU
3. ROI de chaque strat√©gie
4. Recommandations pour futurs entra√Ænements

---

## üéØ Crit√®res de Succ√®s

‚úÖ **Succ√®s** si:
- Les 3 mod√®les convergent sans erreur
- Loss d√©cro√Æt de mani√®re stable
- Chinchilla termine en <8h
- Mod√®les g√©n√®rent du texte coh√©rent

‚ö†Ô∏è **Points d'attention:**
- Divergence de loss
- OOM errors
- Checkpoints corrompus
- Variations GPU trop importantes

---

## üìÅ Structure des R√©sultats

```
checkpoints/
‚îú‚îÄ‚îÄ slimpajama_llama/
‚îÇ   ‚îú‚îÄ‚îÄ model_step_5000.pt
‚îÇ   ‚îú‚îÄ‚îÄ model_step_10000.pt
‚îÇ   ‚îî‚îÄ‚îÄ final_model.pt
‚îú‚îÄ‚îÄ slimpajama_chinchilla/
‚îÇ   ‚îî‚îÄ‚îÄ final_model.pt (plus rapide)
‚îî‚îÄ‚îÄ wikipedia_llama/
    ‚îî‚îÄ‚îÄ final_model.pt

logs/
‚îú‚îÄ‚îÄ slimpajama_llama.log
‚îú‚îÄ‚îÄ slimpajama_chinchilla.log
‚îî‚îÄ‚îÄ wikipedia_llama.log
```

---

## üöÄ Prochaines √âtapes

1. ‚úÖ Entra√Ænements lanc√©s
2. ‚è≥ Monitoring continu (7-35h)
3. üìä Collecte des m√©triques
4. üß™ √âvaluation comparative
5. üìù Rapport final avec recommandations

---

**Date de d√©but:** $(date)  
**Statut:** üü¢ En cours  
**Monitoring:** `./monitor_all_trainings.sh`

