#!/usr/bin/env python3
"""
PrÃ©paration des datasets d'instruction pour fine-tuning conversationnel.

Datasets utilisÃ©s:
1. ShareGPT (~90k conversations)
2. OpenAssistant (~160k messages) 
3. Dolly-15k (instructions diverses)

Total: ~250k exemples de haute qualitÃ©
"""

import json
import os
from pathlib import Path
from typing import List, Dict
from datasets import load_dataset
from tqdm import tqdm
import random


class InstructionDatasetPreparator:
    """PrÃ©pare et formate les datasets d'instruction."""
    
    def __init__(self, output_dir: str = "data/instruction"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def format_conversation(self, messages: List[Dict]) -> str:
        """
        Formate une conversation au format unifiÃ©.
        
        Format:
        User: [message]
        Assistant: [rÃ©ponse]
        User: [message suivant]
        Assistant: [rÃ©ponse suivante]
        """
        formatted = []
        for msg in messages:
            role = msg.get('role', msg.get('from', 'unknown'))
            content = msg.get('content', msg.get('value', ''))
            
            if role in ['user', 'human', 'question']:
                formatted.append(f"User: {content}")
            elif role in ['assistant', 'gpt', 'answer']:
                formatted.append(f"Assistant: {content}")
        
        return '\n'.join(formatted)
    
    def prepare_sharegpt(self) -> List[Dict]:
        """PrÃ©pare ShareGPT dataset."""
        print("\nğŸ“¥ TÃ©lÃ©chargement ShareGPT...")
        
        try:
            # Note: ShareGPT peut nÃ©cessiter un token HuggingFace
            # Alternative: utiliser un subset public
            dataset = load_dataset("anon8231489123/ShareGPT_Vicuna_unfiltered", split="train")
            
            print(f"âœ“ {len(dataset)} conversations ShareGPT chargÃ©es")
            
            formatted_data = []
            for item in tqdm(dataset, desc="Formatage ShareGPT"):
                if 'conversations' in item:
                    text = self.format_conversation(item['conversations'])
                    if len(text) > 50:  # Filtrer conversations trop courtes
                        formatted_data.append({
                            'text': text,
                            'source': 'sharegpt'
                        })
            
            print(f"âœ“ {len(formatted_data)} conversations ShareGPT formatÃ©es")
            return formatted_data
            
        except Exception as e:
            print(f"âš ï¸  Erreur ShareGPT: {e}")
            print("   Continuons avec les autres datasets...")
            return []
    
    def prepare_openassistant(self) -> List[Dict]:
        """PrÃ©pare OpenAssistant dataset."""
        print("\nğŸ“¥ TÃ©lÃ©chargement OpenAssistant...")
        
        try:
            dataset = load_dataset("OpenAssistant/oasst1", split="train")
            print(f"âœ“ {len(dataset)} messages OpenAssistant chargÃ©s")
            
            # OpenAssistant est structurÃ© en arbre de messages
            # On reconstruit les conversations linÃ©aires
            conversations = {}
            
            for item in dataset:
                msg_id = item['message_id']
                parent_id = item['parent_id']
                role = item['role']
                text = item['text']
                
                # Construire des paires question-rÃ©ponse
                if role == 'assistant' and parent_id:
                    # Trouver la question associÃ©e
                    formatted_text = f"User: [question]\nAssistant: {text}"
                    conversations[msg_id] = {
                        'text': formatted_text,
                        'source': 'openassistant'
                    }
            
            formatted_data = list(conversations.values())
            print(f"âœ“ {len(formatted_data)} conversations OpenAssistant formatÃ©es")
            return formatted_data
            
        except Exception as e:
            print(f"âš ï¸  Erreur OpenAssistant: {e}")
            return []
    
    def prepare_dolly(self) -> List[Dict]:
        """PrÃ©pare Dolly-15k dataset."""
        print("\nğŸ“¥ TÃ©lÃ©chargement Dolly-15k...")
        
        try:
            dataset = load_dataset("databricks/databricks-dolly-15k", split="train")
            print(f"âœ“ {len(dataset)} instructions Dolly chargÃ©es")
            
            formatted_data = []
            for item in tqdm(dataset, desc="Formatage Dolly"):
                instruction = item['instruction']
                context = item.get('context', '')
                response = item['response']
                
                # Format avec contexte si disponible
                if context:
                    text = f"User: {instruction}\nContext: {context}\nAssistant: {response}"
                else:
                    text = f"User: {instruction}\nAssistant: {response}"
                
                formatted_data.append({
                    'text': text,
                    'source': 'dolly'
                })
            
            print(f"âœ“ {len(formatted_data)} instructions Dolly formatÃ©es")
            return formatted_data
            
        except Exception as e:
            print(f"âš ï¸  Erreur Dolly: {e}")
            return []
    
    def prepare_alpaca(self) -> List[Dict]:
        """PrÃ©pare Alpaca dataset (fallback si autres Ã©chouent)."""
        print("\nğŸ“¥ TÃ©lÃ©chargement Alpaca...")
        
        try:
            dataset = load_dataset("tatsu-lab/alpaca", split="train")
            print(f"âœ“ {len(dataset)} instructions Alpaca chargÃ©es")
            
            formatted_data = []
            for item in tqdm(dataset, desc="Formatage Alpaca"):
                instruction = item['instruction']
                input_text = item.get('input', '')
                output = item['output']
                
                if input_text:
                    text = f"User: {instruction}\nInput: {input_text}\nAssistant: {output}"
                else:
                    text = f"User: {instruction}\nAssistant: {output}"
                
                formatted_data.append({
                    'text': text,
                    'source': 'alpaca'
                })
            
            print(f"âœ“ {len(formatted_data)} instructions Alpaca formatÃ©es")
            return formatted_data
            
        except Exception as e:
            print(f"âš ï¸  Erreur Alpaca: {e}")
            return []
    
    def prepare_all(self, max_samples: int = None) -> str:
        """
        PrÃ©pare tous les datasets et les combine.
        
        Args:
            max_samples: Limite le nombre total d'exemples (None = pas de limite)
            
        Returns:
            Chemin vers le fichier de donnÃ©es combinÃ©es
        """
        print("="*70)
        print("  ğŸ“š PRÃ‰PARATION DES DATASETS D'INSTRUCTION")
        print("="*70)
        
        all_data = []
        
        # Essayer tous les datasets
        datasets_to_try = [
            ('Alpaca', self.prepare_alpaca),      # Plus fiable, commencer par celui-ci
            ('Dolly', self.prepare_dolly),
            ('OpenAssistant', self.prepare_openassistant),
            ('ShareGPT', self.prepare_sharegpt),
        ]
        
        for name, prepare_func in datasets_to_try:
            try:
                data = prepare_func()
                all_data.extend(data)
            except Exception as e:
                print(f"âš ï¸  Ã‰chec {name}: {e}")
                continue
        
        if not all_data:
            raise ValueError("âŒ Aucun dataset n'a pu Ãªtre chargÃ© !")
        
        # MÃ©langer pour diversitÃ©
        print(f"\nğŸ”€ MÃ©lange de {len(all_data)} exemples...")
        random.shuffle(all_data)
        
        # Limiter si demandÃ©
        if max_samples and len(all_data) > max_samples:
            print(f"âœ‚ï¸  Limitation Ã  {max_samples} exemples")
            all_data = all_data[:max_samples]
        
        # Statistiques par source
        print("\nğŸ“Š Statistiques par source:")
        sources = {}
        for item in all_data:
            source = item['source']
            sources[source] = sources.get(source, 0) + 1
        
        for source, count in sorted(sources.items()):
            print(f"   {source}: {count:,} exemples")
        
        # Split train/validation (95/5)
        split_idx = int(len(all_data) * 0.95)
        train_data = all_data[:split_idx]
        val_data = all_data[split_idx:]
        
        print(f"\nğŸ“‚ CrÃ©ation des fichiers:")
        print(f"   Train: {len(train_data):,} exemples")
        print(f"   Validation: {len(val_data):,} exemples")
        
        # Sauvegarder
        train_file = self.output_dir / "train.jsonl"
        val_file = self.output_dir / "val.jsonl"
        
        with open(train_file, 'w', encoding='utf-8') as f:
            for item in train_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        with open(val_file, 'w', encoding='utf-8') as f:
            for item in val_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        print(f"\nâœ… Datasets sauvegardÃ©s:")
        print(f"   {train_file}")
        print(f"   {val_file}")
        
        # Afficher quelques exemples
        print(f"\nğŸ“ Exemples de donnÃ©es:")
        print("="*70)
        for i, item in enumerate(random.sample(train_data, min(3, len(train_data))), 1):
            print(f"\nExemple {i} ({item['source']}):")
            print("-"*70)
            print(item['text'][:300] + "..." if len(item['text']) > 300 else item['text'])
        print("="*70)
        
        return str(train_file)


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="PrÃ©parer les datasets d'instruction")
    parser.add_argument(
        "--output_dir",
        type=str,
        default="data/instruction",
        help="RÃ©pertoire de sortie"
    )
    parser.add_argument(
        "--max_samples",
        type=int,
        default=None,
        help="Nombre maximum d'exemples (None = tous)"
    )
    
    args = parser.parse_args()
    
    preparator = InstructionDatasetPreparator(output_dir=args.output_dir)
    
    try:
        train_file = preparator.prepare_all(max_samples=args.max_samples)
        
        print("\n" + "="*70)
        print("  âœ… PRÃ‰PARATION TERMINÃ‰E")
        print("="*70)
        print(f"\nğŸ’¡ Prochaine Ã©tape:")
        print(f"   python finetune.py --train_file {train_file}")
        print()
        
    except Exception as e:
        print(f"\nâŒ Erreur: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

