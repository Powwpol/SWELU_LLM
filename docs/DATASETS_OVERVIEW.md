# Datasets Disponibles pour SWELU LLM

## üìä Tailles Estim√©es et Sources

### üî¢ MATH√âMATIQUES

| Dataset | Taille | Tokens (estim√©) | Qualit√© | Disponibilit√© |
|---------|--------|-----------------|---------|---------------|
| **MathPile** | 12.7GB | ~3.2B tokens | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ HuggingFace |
| **ArXiv Math** | ~15GB | ~3.8B tokens | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ HuggingFace |
| **Proof-Pile-2** | 15GB | ~3.8B tokens | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ HuggingFace |
| **ProofWiki** | ~500MB | ~125M tokens | ‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è Scraping requis |
| **Khan Academy** | ~2GB | ~500M tokens | ‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è API/scraping |
| **TOTAL MATHS** | **~45GB** | **~11.4B tokens** | | |

### üî¨ LEAN & FORMAL MATH

| Dataset | Taille | Tokens (estim√©) | Qualit√© | Disponibilit√© |
|---------|--------|-----------------|---------|---------------|
| **Lean Mathlib** | ~2GB | ~500M tokens | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ GitHub clone |
| **Lean 4 Examples** | ~100MB | ~25M tokens | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ GitHub |
| **Coq Standard Lib** | ~500MB | ~125M tokens | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ GitHub |
| **Isabelle Archive** | ~1GB | ~250M tokens | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ Disponible |
| **TOTAL LEAN** | **~3.6GB** | **~900M tokens** | | |

### üì¶ SUPPLY CHAIN (‚ö†Ô∏è PROBL√àME)

| Dataset | Taille | Tokens (estim√©) | Qualit√© | Disponibilit√© |
|---------|--------|-----------------|---------|---------------|
| **Financial News** | ~500MB | ~125M tokens | ‚≠ê‚≠ê | ‚úÖ HuggingFace |
| **Business Reports** | ~1GB | ~250M tokens | ‚≠ê‚≠ê | ‚ö†Ô∏è Scraping |
| **Supply Chain Blogs** | ~200MB | ~50M tokens | ‚≠ê | ‚ö†Ô∏è Scraping |
| **Kaggle SC Datasets** | ~10MB | ~2.5M tokens | ‚≠ê | ‚úÖ Kaggle |
| **TOTAL SC** | **~1.7GB** | **~427M tokens** | ‚ö†Ô∏è Faible qualit√© | |

**‚ö†Ô∏è PROBL√àME CRITIQUE: Pas de grand dataset public Supply Chain!**

Options pour Supply Chain:
1. **Scraping web** (100-500MB possible)
2. **G√©n√©ration synth√©tique** (qualit√© moyenne)
3. **Fine-tuning ult√©rieur** sur donn√©es propri√©taires (recommand√©)

## üéØ Strat√©gie Recommand√©e

### Phase 1: Base G√©n√©rale + Maths
```
Wikipedia (6GB)           ~1.5B tokens
MathPile (12.7GB)        ~3.2B tokens
Proof-Pile-2 (15GB)      ~3.8B tokens
TOTAL: 33.7GB            ~8.5B tokens
```

**Dur√©e download:** ~2-3h (selon connexion)  
**Espace disque:** ~70GB (raw + processed)

### Phase 2: Ajout Lean
```
Lean Mathlib (2GB)       ~500M tokens
Lean Examples (100MB)    ~25M tokens
TOTAL: +2.1GB            +525M tokens
```

**Total cumul√©:** ~35.8GB, ~9B tokens

### Phase 3: Supply Chain (limit√©)
```
Financial/Business       ~427M tokens
Web scraping custom      ~50-100M tokens
TOTAL: +1.7GB            +527M tokens
```

**TOTAL FINAL:** ~37.5GB raw, **~9.5B tokens**

## üíæ Espace Disque Requis

| √âtape | Espace |
|-------|--------|
| Download raw data | ~40GB |
| Processed tokens | ~30GB |
| Checkpoints (training) | ~20GB |
| **TOTAL** | **~90GB** |

## ‚è±Ô∏è Temps de T√©l√©chargement

| Connexion | Temps (40GB) |
|-----------|--------------|
| 10 Mbps | ~9h |
| 50 Mbps | ~2h |
| 100 Mbps | ~1h |
| 1 Gbps | ~6min |

## üöÄ Commandes pour T√©l√©charger

### Option 1: Tout t√©l√©charger (recommand√© pour production)

```bash
# T√©l√©charger tous les datasets sp√©cialis√©s
python src/data/prepare_specialized_datasets.py \
  --domain all \
  --output data/specialized

# Temps estim√©: 2-4h
# Espace requis: ~90GB
```

### Option 2: Test rapide (petit √©chantillon)

```bash
# T√©l√©charger √©chantillons uniquement
python src/data/prepare_specialized_datasets.py \
  --domain all \
  --max_samples 1000 \
  --output data/specialized_test

# Temps: ~15min
# Espace: ~1GB
```

### Option 3: Par domaine

```bash
# Seulement maths
python src/data/prepare_specialized_datasets.py --domain math

# Seulement Lean
python src/data/prepare_specialized_datasets.py --domain lean

# Seulement Supply Chain (limit√©)
python src/data/prepare_specialized_datasets.py --domain supply_chain
```

## üìà Comparaison avec LLMs Existants

| Mod√®le | Tokens Training | Notre Target |
|--------|-----------------|--------------|
| GPT-3 | 300B | ‚ùå Trop ambitieux |
| LLaMA-7B | 1T | ‚ùå Impossible |
| Pythia-410M | 300B | ‚ùå Trop |
| **SWELU (350M)** | **~10B** | ‚úÖ R√©aliste |

**Notre cible:** 10B tokens = raisonnable pour mod√®le 350M

## ‚ö†Ô∏è Limitations Actuelles

### Supply Chain
- **Probl√®me:** Pas de dataset public massif
- **Impact:** Mod√®le sera faible sur SC
- **Solution:** Fine-tuning post-training sur donn√©es propri√©taires

### Lean
- **Probl√®me:** Syntaxe tr√®s sp√©cifique
- **Impact:** Tokenizer GPT-2 pas optimal
- **Solution:** Consid√©rer tokenizer custom pour Lean

### Maths
- **OK:** Beaucoup de donn√©es disponibles
- **Qualit√©:** Excellente (papers acad√©miques)

## üéØ Recommandation Finale

**Pour training complet (40h sur RTX 4090):**

1. **T√©l√©charge maintenant** (pendant que tu dors):
   ```bash
   nohup python src/data/prepare_specialized_datasets.py --domain all > download.log 2>&1 &
   ```

2. **Utilise 80/20:**
   - 80% Maths (MathPile + Proof-Pile + ArXiv)
   - 15% Lean (Mathlib)
   - 5% Supply Chain (ce qu'on a)

3. **Pr√©voir fine-tuning ult√©rieur** sur donn√©es SC propri√©taires

## üí∞ Co√ªt Storage RunPod

| Volume | Co√ªt/mois |
|--------|-----------|
| 50GB | $5/mois |
| 100GB | $10/mois |
| 200GB | $20/mois |

**Recommandation:** Volume 100GB sur RunPod = $10/mois

---

**Tu veux lancer le t√©l√©chargement maintenant?**

```bash
# Test rapide (1000 samples, ~15min)
python src/data/prepare_specialized_datasets.py --domain all --max_samples 1000

# OU full download (2-4h)
python src/data/prepare_specialized_datasets.py --domain all
```

