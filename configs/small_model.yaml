# Configuration pour petit modèle de test (GPU)
# Durée estimée: ~10min sur RTX 4090
# Mémoire GPU: ~2-3GB

model:
  vocab_size: 50257       # GPT-2 tokenizer
  d_model: 256            # Réduit de 1024 pour test
  n_layers: 2             # Réduit de 6 pour test
  max_seq_len: 512        # Réduit de 2048 pour test
  d_state: 16             # Mamba state dimension
  d_conv: 4               # Mamba conv dimension
  expand_factor: 2        # Mamba expansion factor

training:
  # Data
  dataset: wikipedia
  tokenizer: gpt2
  max_samples: 10000      # Sous-ensemble pour test rapide
  
  # Optimization
  batch_size: 4
  gradient_accumulation_steps: 1
  learning_rate: 3.0e-4
  weight_decay: 0.1
  warmup_steps: 100
  max_steps: 1000         # ~10min
  
  # Precision
  mixed_precision: bf16   # bf16, fp16, ou none
  
  # Logging & Checkpointing
  log_every: 50
  eval_every: 200
  checkpoint_every: 500
  checkpoint_dir: ./checkpoints_small
  
  # Weights & Biases (optionnel)
  use_wandb: false
  wandb_project: mamba-swelu-test
  
  # DataLoader
  num_workers: 2
  pin_memory: true

