# Configuration pour modèle complet sur RunPod
# Instance recommandée: RTX 4090 (24GB VRAM)
# Durée estimée: ~40h pour 100k steps
# Coût estimé: ~16$ (0.40$/h × 40h)

model:
  vocab_size: 50257       # GPT-2 tokenizer
  d_model: 1024           # Model dimension
  n_layers: 6             # Number of Mamba layers
  max_seq_len: 2048       # Maximum sequence length
  d_state: 16             # Mamba state dimension
  d_conv: 4               # Mamba conv dimension
  expand_factor: 2        # Mamba expansion factor

training:
  # Data
  dataset: wikipedia      # wikipedia ou c4
  tokenizer: gpt2
  max_samples: null       # null = utiliser tout le dataset
  
  # Optimization
  batch_size: 8
  gradient_accumulation_steps: 4  # Effective batch = 8 × 4 = 32
  learning_rate: 3.0e-4
  weight_decay: 0.1
  warmup_steps: 2000
  max_steps: 100000
  
  # Precision
  mixed_precision: bf16   # bf16 recommandé pour RTX 4090
  
  # Logging & Checkpointing
  log_every: 100
  eval_every: 1000
  checkpoint_every: 5000
  checkpoint_dir: ./checkpoints
  
  # Weights & Biases
  use_wandb: true
  wandb_project: mamba-swelu
  wandb_entity: null      # Votre username W&B
  
  # DataLoader
  num_workers: 4
  pin_memory: true
  
  # RunPod specific
  save_to_cloud: true     # Sync checkpoints vers cloud storage
  cloud_sync_every: 10000 # Sync tous les 10k steps

