# ğŸ¯ StratÃ©gie de Fine-Tuning pour MambaSWELU

## ğŸ“Š Diagnostic Actuel

**ModÃ¨le** : MambaSWELU 124M paramÃ¨tres
**Training** : 757,500 steps sur SlimPajama
**ProblÃ¨mes identifiÃ©s** :
- âŒ Pas de comprÃ©hension des questions
- âŒ RÃ©ponses hors-sujet systÃ©matiques
- âŒ Hallucinations (capitale de France = "Tijdens")
- âŒ RÃ©pÃ©titions en boucle
- âŒ MÃ©lange code/texte incohÃ©rent

**Cause racine** : ModÃ¨le base sans instruction tuning

---

## ğŸš€ 3 StratÃ©gies ProposÃ©es

### **Option 1 : Fine-Tuning Rapide (1-2 jours)** âš¡
*Pour obtenir rapidement un modÃ¨le conversationnel basique*

**Dataset** : Alpaca (52k instructions en anglais)
- Format : `instruction` + `input` + `output`
- Taille : ~50MB
- Temps d'entraÃ®nement : 5-10k steps (~6h sur 6x RTX 4090)

**HyperparamÃ¨tres recommandÃ©s** :
```bash
--learning_rate 1e-5          # Plus bas que prÃ©-training
--weight_decay 0.01           # RÃ©duit pour ne pas casser le modÃ¨le
--warmup_steps 500            # Court warmup
--max_steps 10000             # 10k steps suffisent
--batch_size 4                # Plus petit pour stabilitÃ©
--gradient_accumulation 8     # Total batch = 32
--checkpoint_every 1000
```

**Avantages** :
- âœ… Rapide Ã  mettre en place
- âœ… Dataset propre et testÃ©
- âœ… RÃ©sultats visibles en quelques heures

**InconvÃ©nients** :
- âš ï¸ Seulement en anglais
- âš ï¸ QualitÃ© moyenne (dataset de 2023)

---

### **Option 2 : Fine-Tuning Conversationnel (3-5 jours)** ğŸ¯
*Pour un modÃ¨le chat de meilleure qualitÃ©*

**Datasets combinÃ©s** :
1. **ShareGPT** (~90k conversations)
2. **OpenAssistant** (~160k messages)
3. **Dolly-15k** (instructions diverses)

Total : ~250k exemples de qualitÃ©

**Format unifiÃ©** :
```
User: [question/instruction]
Assistant: [rÃ©ponse]
```

**HyperparamÃ¨tres** :
```bash
--learning_rate 5e-6          # TrÃ¨s bas pour ne pas oublier
--weight_decay 0.05
--warmup_steps 1000
--max_steps 25000             # ~20h sur 6 GPUs
--batch_size 2
--gradient_accumulation 16    # Total batch = 32
--max_seq_len 1024            # Conversations plus longues
```

**Avantages** :
- âœ… Meilleure qualitÃ© conversationnelle
- âœ… Multi-tour (conversations)
- âœ… Datasets variÃ©s

**InconvÃ©nients** :
- âš ï¸ Plus long Ã  prÃ©parer
- âš ï¸ NÃ©cessite preprocessing

---

### **Option 3 : Fine-Tuning SpÃ©cialisÃ© (1-2 semaines)** ğŸ†
*Pour un modÃ¨le expert dans un domaine*

**Choix du domaine** :
1. **Code** : CodeAlpaca + StackOverflow filtered
2. **Science** : ArXiv papers + PubMed
3. **FranÃ§ais** : Datasets francophones (Fleurs, CulturaX)

**Approche en 2 phases** :
1. **Phase 1** : Instruction tuning gÃ©nÃ©ral (Option 2)
2. **Phase 2** : SpÃ©cialisation domaine (15k-30k steps)

**Avantages** :
- âœ… Expertise de niche
- âœ… Meilleure performance sur cas d'usage ciblÃ©

**InconvÃ©nients** :
- âš ï¸ Long Ã  entraÃ®ner
- âš ï¸ Perte de gÃ©nÃ©ralitÃ©

---

## ğŸ¯ Recommandation PersonnalisÃ©e

**JE RECOMMANDE : Option 2 (Conversationnel)**

**Pourquoi ?**
1. Tu as 6x RTX 4090 â†’ capacitÃ© suffisante
2. Tu veux un modÃ¨le chat fonctionnel
3. Compromis temps/qualitÃ© optimal
4. Datasets de qualitÃ© disponibles

**Plan d'action concret** :

### ğŸ“… Timeline (5 jours)

**Jour 1** : PrÃ©paration donnÃ©es
- TÃ©lÃ©charger ShareGPT + OpenAssistant
- CrÃ©er script de preprocessing
- Formater en prompt conversationnel

**Jour 2** : Setup fine-tuning
- Adapter train.py pour instruction tuning
- Tester sur 1 GPU (validation)
- VÃ©rifier que Ã§a ne crash pas

**Jour 3-4** : Training
- Lancer sur 6 GPUs
- 25k steps (~20h)
- Monitoring toutes les 2h

**Jour 5** : Ã‰valuation
- Tests qualitatifs (comme demo_chat.py)
- Comparaison avant/aprÃ¨s
- ItÃ©ration si nÃ©cessaire

---

## ğŸ› ï¸ Scripts Ã  CrÃ©er

### 1. `prepare_instruction_data.py`
TÃ©lÃ©charge et formate les datasets

### 2. `finetune.py`
Script de fine-tuning adaptÃ© (learning rate bas, etc.)

### 3. `compare_models.py`
Compare modÃ¨le base vs fine-tunÃ©

### 4. `benchmark.py`
MÃ©triques quantitatives (perplexity, BLEU, etc.)

---

## ğŸ’¡ Conseils Critiques

1. **Learning Rate** : TRÃˆS IMPORTANT
   - Trop haut â†’ catastrophic forgetting
   - Trop bas â†’ pas d'apprentissage
   - **Optimal : 1e-5 Ã  5e-6**

2. **Gradient Accumulation**
   - Ton modÃ¨le = 124M params â†’ ~500MB en FP16
   - 6x RTX 4090 (24GB chacun)
   - **Tu peux faire batch_size=4 par GPU = 24 total**

3. **Checkpointing**
   - Sauvegarde **TOUS les 1000 steps**
   - Garde les 5 derniers checkpoints
   - Teste rÃ©guliÃ¨rement avec demo_chat.py

4. **Monitoring**
   - Loss doit descendre graduellement
   - Si loss augmente â†’ learning rate trop haut
   - Si loss stagne â†’ peut-Ãªtre terminÃ©

---

## ğŸš¨ Erreurs Ã  Ã‰viter

1. âŒ **Ne pas partir de zÃ©ro** : Utilise ton checkpoint actuel
2. âŒ **Ne pas utiliser adam normal** : Utilise AdamW
3. âŒ **Ne pas oublier warmup** : Sinon instabilitÃ©
4. âŒ **Ne pas fine-tuner trop longtemps** : Risque d'overfitting
5. âŒ **Ne pas tester en cours de route** : VÃ©rifie Ã  5k, 10k, 15k, 20k steps

---

## ğŸ“Š MÃ©triques de SuccÃ¨s

**AprÃ¨s fine-tuning, ton modÃ¨le devrait** :

âœ… RÃ©pondre "Paris" Ã  "capitale de France"
âœ… Calculer 2+2=4
âœ… Maintenir le contexte conversationnel
âœ… Ne plus halluciner des noms alÃ©atoires
âœ… Suivre les instructions simples

**Si Ã©chec** :
- RÃ©duire learning rate Ã· 2
- Augmenter steps (jusqu'Ã  50k)
- Changer de dataset

---

## ğŸ¬ PrÃªt Ã  Commencer ?

**Tu veux que je** :
1. ğŸš€ CrÃ©e les scripts de fine-tuning (Option 2)
2. ğŸ“¥ PrÃ©pare le tÃ©lÃ©chargement des datasets
3. ğŸ§ª Setup un test rapide sur 1 GPU d'abord
4. ğŸ“Š Autre chose ?

**Dis-moi et on y va !** ğŸ’ª

