#!/bin/bash
# Lance 6 entraÃ®nements SlimPajama INDÃ‰PENDANTS sur 6 GPUs
# Chaque GPU entraÃ®ne son propre modÃ¨le â†’ 6 modÃ¨les diffÃ©rents Ã  comparer!

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  ğŸš€ 6 MODÃˆLES SLIMPAJAMA INDÃ‰PENDANTS - 1 PAR GPU"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "  StratÃ©gie: 6 GPUs en parallÃ¨le, 6 modÃ¨les indÃ©pendants"
echo ""
echo "  Avantages:"
echo "    âœ… Pas de complications DDP"
echo "    âœ… 6 modÃ¨les Ã  comparer (variations alÃ©atoires)"
echo "    âœ… Robustesse: si 1 crash, les autres continuent"
echo "    âœ… FlexibilGPUitÃ©: diffÃ©rents hyperparamÃ¨tres possibles"
echo ""
echo "  Configuration par modÃ¨le:"
echo "    - Steps:   757,500 (LLaMA 100x)"
echo "    - DurÃ©e:   ~70h par GPU"
echo "    - Tokens:  12.4B chacun"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

cd /root/SWELU_LLM

# Charger token
export $(cat .env | xargs)

# ArrÃªter tout
pkill -f train.py 2>/dev/null
sleep 3

# CrÃ©er rÃ©pertoires
for i in {0..5}; do
    mkdir -p checkpoints/model_gpu$i
    mkdir -p logs
done

echo "ğŸš€ Lancement des 6 modÃ¨les..."
echo ""

# GPU 0
CUDA_VISIBLE_DEVICES=0 HF_TOKEN=$HF_TOKEN \
nohup python src/train.py \
  --dataset slimpajama --vocab_size 50257 --d_model 1024 --n_layers 6 \
  --max_seq_len 1024 --batch_size 4 --gradient_accumulation_steps 4 \
  --max_steps 757500 --learning_rate 3e-4 --weight_decay 0.1 \
  --warmup_steps 2000 --mixed_precision bf16 \
  --checkpoint_dir ./checkpoints/model_gpu0 --checkpoint_every 5000 --log_every 100 \
  > logs/gpu0.log 2>&1 &
echo "   GPU 0: PID $! â†’ checkpoints/model_gpu0/"

# GPU 1  
CUDA_VISIBLE_DEVICES=1 HF_TOKEN=$HF_TOKEN \
nohup python src/train.py \
  --dataset slimpajama --vocab_size 50257 --d_model 1024 --n_layers 6 \
  --max_seq_len 1024 --batch_size 4 --gradient_accumulation_steps 4 \
  --max_steps 757500 --learning_rate 3e-4 --weight_decay 0.1 \
  --warmup_steps 2000 --mixed_precision bf16 \
  --checkpoint_dir ./checkpoints/model_gpu1 --checkpoint_every 5000 --log_every 100 \
  > logs/gpu1.log 2>&1 &
echo "   GPU 1: PID $! â†’ checkpoints/model_gpu1/"

# GPU 2
CUDA_VISIBLE_DEVICES=2 HF_TOKEN=$HF_TOKEN \
nohup python src/train.py \
  --dataset slimpajama --vocab_size 50257 --d_model 1024 --n_layers 6 \
  --max_seq_len 1024 --batch_size 4 --gradient_accumulation_steps 4 \
  --max_steps 757500 --learning_rate 3e-4 --weight_decay 0.1 \
  --warmup_steps 2000 --mixed_precision bf16 \
  --checkpoint_dir ./checkpoints/model_gpu2 --checkpoint_every 5000 --log_every 100 \
  > logs/gpu2.log 2>&1 &
echo "   GPU 2: PID $! â†’ checkpoints/model_gpu2/"

# GPU 3
CUDA_VISIBLE_DEVICES=3 HF_TOKEN=$HF_TOKEN \
nohup python src/train.py \
  --dataset slimpajama --vocab_size 50257 --d_model 1024 --n_layers 6 \
  --max_seq_len 1024 --batch_size 4 --gradient_accumulation_steps 4 \
  --max_steps 757500 --learning_rate 3e-4 --weight_decay 0.1 \
  --warmup_steps 2000 --mixed_precision bf16 \
  --checkpoint_dir ./checkpoints/model_gpu3 --checkpoint_every 5000 --log_every 100 \
  > logs/gpu3.log 2>&1 &
echo "   GPU 3: PID $! â†’ checkpoints/model_gpu3/"

# GPU 4
CUDA_VISIBLE_DEVICES=4 HF_TOKEN=$HF_TOKEN \
nohup python src/train.py \
  --dataset slimpajama --vocab_size 50257 --d_model 1024 --n_layers 6 \
  --max_seq_len 1024 --batch_size 4 --gradient_accumulation_steps 4 \
  --max_steps 757500 --learning_rate 3e-4 --weight_decay 0.1 \
  --warmup_steps 2000 --mixed_precision bf16 \
  --checkpoint_dir ./checkpoints/model_gpu4 --checkpoint_every 5000 --log_every 100 \
  > logs/gpu4.log 2>&1 &
echo "   GPU 4: PID $! â†’ checkpoints/model_gpu4/"

# GPU 5
CUDA_VISIBLE_DEVICES=5 HF_TOKEN=$HF_TOKEN \
nohup python src/train.py \
  --dataset slimpajama --vocab_size 50257 --d_model 1024 --n_layers 6 \
  --max_seq_len 1024 --batch_size 4 --gradient_accumulation_steps 4 \
  --max_steps 757500 --learning_rate 3e-4 --weight_decay 0.1 \
  --warmup_steps 2000 --mixed_precision bf16 \
  --checkpoint_dir ./checkpoints/model_gpu5 --checkpoint_every 5000 --log_every 100 \
  > logs/gpu5.log 2>&1 &
echo "   GPU 5: PID $! â†’ checkpoints/model_gpu5/"

sleep 5

echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "âœ… 6 MODÃˆLES LANCÃ‰S - 1 PAR GPU!"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader,nounits | \
    awk -F', ' '{printf "   GPU %s: Util=%3s%%, Mem=%5sMB\n", $1, $2, $3}'
echo ""
echo "ğŸ“ Logs individuels:"
echo "   tail -f logs/gpu0.log"
echo "   tail -f logs/gpu1.log"
echo "   ... gpu2, gpu3, gpu4, gpu5 ..."
echo ""
echo "ğŸ“Š Monitoring global:"
echo "   watch -n 10 'nvidia-smi'"
echo ""
echo "ğŸ’¾ 6 sets de checkpoints:"
echo "   checkpoints/model_gpu0/"
echo "   checkpoints/model_gpu1/"
echo "   ... jusqu'Ã  model_gpu5/"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

